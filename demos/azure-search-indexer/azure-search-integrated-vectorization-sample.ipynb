{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search integrated vectorization sample\n",
    "This code demonstrates how to use Azure AI Search as a vector store by automatically chunking and generating embeddings using the AzureOpenAIEmbedding skill as part of the skillset pipeline in Azure AI Search. \n",
    "## Prerequisites\n",
    "To run the code, install the following packages. This sample currently uses version `11.4.0b12` which is a pre-release version. Please note, that integrated vectorization feature is in preview and has not been published to [azure-search-documents](https://pypi.org/project/azure-search-documents/#description) on pypi. If you'd like to use this feature, please reference the whl file. We hope to publish an updated version soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ../whl/azure_search_documents-11.4.0b12-py3-none-any.whl --quiet  \n",
    "! pip install openai azure-storage-blob python-dotenv --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    RawVectorQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    AzureOpenAIEmbeddingSkill,  \n",
    "    AzureOpenAIParameters,  \n",
    "    AzureOpenAIVectorizer,  \n",
    "    ExhaustiveKnnParameters,  \n",
    "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "    FieldMapping,\n",
    "    FieldMappingFunction,\n",
    "    HnswParameters,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    "    IndexProjectionMode,  \n",
    "    InputFieldMappingEntry,  \n",
    "    OutputFieldMappingEntry,  \n",
    "    PrioritizedFields,    \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SearchIndex,  \n",
    "    SearchIndexer,  \n",
    "    SearchIndexerDataContainer,  \n",
    "    SearchIndexerDataSourceConnection,  \n",
    "    SearchIndexerIndexProjectionSelector,  \n",
    "    SearchIndexerIndexProjections,  \n",
    "    SearchIndexerIndexProjectionsParameters,  \n",
    "    SearchIndexerSkillset,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,  \n",
    "    SplitSkill,  \n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmKind,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,  \n",
    ")  \n",
    "from azure.storage.blob import BlobServiceClient  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "  \n",
    "# Configure environment variables  \n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")  \n",
    "model: str = \"text-embedding-ada-002\"  \n",
    "blob_connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")  \n",
    "container_name = os.getenv(\"BLOB_CONTAINER_NAME\")  \n",
    "credential = AzureKeyCredential(key)\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Blob Storage  \n",
    "Retrieve documents from Blob Storage. You can use the sample documents in the [documents](../data/documents) folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL of the first blob: https://documentstorageaiasbx.blob.core.windows.net/hierarchydocuments/rhdocuments/perks/PerksPlus.pdf\n"
     ]
    }
   ],
   "source": [
    "# Connect to Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "first_blob = next(blobs)\n",
    "blob_url = container_client.get_blob_client(first_blob).url\n",
    "print(f\"URL of the first blob: {blob_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect your Blob storage to a data source in Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'integrated-vectorization-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "# Create a data source \n",
    "ds_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))\n",
    "container = SearchIndexerDataContainer(name=container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=blob_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrated-vectorization created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)  \n",
    "fields = [  \n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"metadata_storage_path\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile=\"myHnswProfile\"),  \n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswVectorSearchAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\",  \n",
    "            kind=VectorSearchAlgorithmKind.HNSW,  \n",
    "            parameters=HnswParameters(  \n",
    "                m=4,  \n",
    "                ef_construction=400,  \n",
    "                ef_search=500,  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "        ExhaustiveKnnVectorSearchAlgorithmConfiguration(  \n",
    "            name=\"myExhaustiveKnn\",  \n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,  \n",
    "            parameters=ExhaustiveKnnParameters(  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myExhaustiveKnnProfile\",  \n",
    "            algorithm=\"myExhaustiveKnn\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    "                deployment_id=model,  \n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=PrioritizedFields(  \n",
    "        prioritized_content_fields=[SemanticField(field_name=\"chunk\")]  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "# Create the semantic settings with the configuration  \n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])  \n",
    "  \n",
    "# Create the search index with the semantic settings  \n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_settings=semantic_settings)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrated-vectorization-skillset created\n"
     ]
    }
   ],
   "source": [
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"  \n",
    "  \n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2048,  \n",
    "    page_overlap_length=20,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    "    deployment_id=model,  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
    "                InputFieldMappingEntry(name=\"metadata_storage_path\", source=\"/document/metadata_metadata_storage_path\"),\n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, embedding_skill],  \n",
    "    index_projections=index_projections,  \n",
    ")  \n",
    "  \n",
    "client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " integrated-vectorization-indexer created\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,  \n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\"),\n",
    "                    FieldMapping(source_field_name=\"metadata_metadata_storage_path\", target_field_name=\"metadata_storage_path\", \n",
    "                                 mapping_function=FieldMappingFunction(name='extractTokenAtPosition', parameters={\"delimiter\": \"/\", \"position\": 5}))]  \n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(service_endpoint, AzureKeyCredential(key))  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f' {indexer_name} created')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a hybrid search + semantic reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"What does a product manager do\"\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector_query = VectorizableTextQuery(text=query, k=2, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    #filter = \"metadata_storage_path like '*hierarchydocuments/rhdocuments/role_library/*'\",\n",
    "    filter = \"metadata_storage_path ge 'https://documentstorageaiasbx.blob.core.windows.net/hierarchydocuments/rhdocuments/perks' and metadata_storage_path lt 'https://documentstorageaiasbx.blob.core.windows.net/hierarchydocuments/rhdocuments/perkt'\",\n",
    "    vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    query_type=QueryType.SEMANTIC, query_language=QueryLanguage.EN_US, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_id: aHR0cHM6Ly9kb2N1bWVudHN0b3JhZ2VhaWFzYnguYmxvYi5jb3JlLndpbmRvd3MubmV0L2hpZXJhcmNoeWRvY3VtZW50cy9yaGRvY3VtZW50cy9wZXJrcy9QZXJrc1BsdXMucGRm0\n",
      "chunk_id: 922bc8dd9e7b_aHR0cHM6Ly9kb2N1bWVudHN0b3JhZ2VhaWFzYnguYmxvYi5jb3JlLndpbmRvd3MubmV0L2hpZXJhcmNoeWRvY3VtZW50cy9yaGRvY3VtZW50cy9wZXJrcy9QZXJrc1BsdXMucGRm0_pages_1\n",
      "Reranker Score: 0.11366414278745651\n",
      "Content: lessons \n",
      "\n",
      "• Scuba diving lessons \n",
      "\n",
      "• Surfing lessons \n",
      "\n",
      "• Horseback riding lessons \n",
      "\n",
      "These lessons provide employees with the opportunity to try new things, challenge themselves, and \n",
      "\n",
      "improve their physical skills. They are also a great way to relieve stress and have fun while staying active. \n",
      "\n",
      "With PerksPlus, employees can choose from a variety of fitness programs to suit their individual needs \n",
      "\n",
      "and preferences. Whether you're looking to improve your physical fitness, reduce stress, or just have \n",
      "\n",
      "some fun, PerksPlus has you covered. \n",
      "\n",
      "What is Not Covered? \n",
      "In addition to the wide range of activities covered by PerksPlus, there is also a list of things that are not \n",
      "\n",
      "covered under the program. These include but are not limited to: \n",
      "\n",
      "• Non-fitness related expenses \n",
      "\n",
      "• Medical treatments and procedures \n",
      "\n",
      "• Travel expenses (unless related to a fitness program) \n",
      "\n",
      "\n",
      "\n",
      "• Food and supplements\n",
      "Caption: lessons   •<em> Scuba diving lessons</em>   • Surfing lessons   •<em> Horseback riding lessons</em>   These lessons provide employees with the opportunity to try new things, challenge themselves, and   improve their physical skills. They are also a great way to relieve stress and have fun while staying active.\n",
      "\n",
      "parent_id: aHR0cHM6Ly9kb2N1bWVudHN0b3JhZ2VhaWFzYnguYmxvYi5jb3JlLndpbmRvd3MubmV0L2hpZXJhcmNoeWRvY3VtZW50cy9yaGRvY3VtZW50cy9wZXJrcy9QZXJrc1BsdXMucGRm0\n",
      "chunk_id: 922bc8dd9e7b_aHR0cHM6Ly9kb2N1bWVudHN0b3JhZ2VhaWFzYnguYmxvYi5jb3JlLndpbmRvd3MubmV0L2hpZXJhcmNoeWRvY3VtZW50cy9yaGRvY3VtZW50cy9wZXJrcy9QZXJrc1BsdXMucGRm0_pages_0\n",
      "Reranker Score: 0.08762247860431671\n",
      "Content: PerksPlus Health and Wellness \n",
      "\n",
      "Reimbursement Program for \n",
      "\n",
      "Contoso Electronics Employees \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "This document contains information generated using a language model (Azure OpenAI). The information \n",
      "\n",
      "contained in this document is only for demonstration purposes and does not reflect the opinions or \n",
      "\n",
      "beliefs of Microsoft. Microsoft makes no representations or warranties of any kind, express or implied, \n",
      "\n",
      "about the completeness, accuracy, reliability, suitability or availability with respect to the information \n",
      "\n",
      "contained in this document.  \n",
      "\n",
      "All rights reserved to Microsoft \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "Overview \n",
      "Introducing PerksPlus - the ultimate benefits program designed to support the health and wellness of \n",
      "\n",
      "employees. With PerksPlus, employees have the opportunity to expense up to $1000 for fitness-related \n",
      "\n",
      "programs, making it easier and more affordable to maintain a healthy lifestyle. PerksPlus is not only \n",
      "\n",
      "designed to support employees' physical health, but also their mental health. Regular exercise has been \n",
      "\n",
      "shown to reduce stress, improve mood, and enhance overall well-being. With PerksPlus, employees can \n",
      "\n",
      "invest in their health and wellness, while enjoying the peace of mind that comes with knowing they are \n",
      "\n",
      "getting the support they need to lead a healthy life. \n",
      "\n",
      "What is Covered? \n",
      "PerksPlus covers a wide range of fitness activities, including but not limited to: \n",
      "\n",
      "• Gym memberships \n",
      "\n",
      "• Personal training sessions \n",
      "\n",
      "• Yoga and Pilates classes \n",
      "\n",
      "• Fitness equipment purchases \n",
      "\n",
      "• Sports team fees \n",
      "\n",
      "• Health retreats and spas \n",
      "\n",
      "• Outdoor adventure activities (such as rock climbing, hiking, and kayaking) \n",
      "\n",
      "• Group fitness classes (such as dance, martial arts, and cycling) \n",
      "\n",
      "• Virtual fitness programs (such as online yoga and workout classes) \n",
      "\n",
      "In addition to the wide range of fitness activities covered by PerksPlus, the program also covers a variety \n",
      "\n",
      "of lessons and experiences that promote health and wellness. Some of the lessons covered under \n",
      "\n",
      "PerksPlus include: \n",
      "\n",
      "• Skiing and snowboarding lessons\n",
      "Caption: perksplus covers a wide range of<em> fitness</em> activities, including but not limited to:   • gym memberships   • personal training sessions   • yoga and pilates classes   •<em> fitness</em> equipment purchases   • sports team fees   • health retreats and spas   • outdoor adventure activities (such as rock climbing, hiking, and kayaking)   • group<em> fitness</em> …\n",
      "\n"
     ]
    }
   ],
   "source": [
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['chunk']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
